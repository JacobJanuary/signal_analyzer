#!/usr/bin/env python3
"""
Full Signals Processor - –û–±—Ä–∞–±–æ—Ç–∫–∞ –í–°–ï–• —Å–∏–≥–Ω–∞–ª–æ–≤ —Å –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ ML –º–æ–¥–µ–ª—è–º–∏
"""

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Tuple, Optional, Any
from dotenv import load_dotenv
from urllib.parse import quote_plus
import warnings

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
import xgboost as xgb
import lightgbm as lgb
from sklearn.neural_network import MLPClassifier
import joblib
from tqdm import tqdm

# –£–õ–£–ß–®–ï–ù–û: –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞—Ç—å –≤—Å–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è.
# –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è, –∏—Ö –º–æ–∂–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–µ—á–Ω–æ.
# warnings.filterwarnings('ignore')

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –£–õ–£–ß–®–ï–ù–û: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è "–º–∞–≥–∏—á–µ—Å–∫–∏—Ö —á–∏—Å–µ–ª"
AVOID_ZERO_DIV_SMALL = 1e-5
AVOID_ZERO_DIV_TINY = 1e-9


class FullSignalsProcessor:
    """
    –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å–∏–≥–Ω–∞–ª–æ–≤ —Å ML –º–æ–¥–µ–ª—è–º–∏.

    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª–æ–≤, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Ö —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
    –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∏ –æ–±—É—á–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å ML-–º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –±—É–¥—É—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤.
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞."""
        self.engine = self._create_db_connection()

        # –£–õ–£–ß–®–ï–ù–û: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—ã–Ω–µ—Å–µ–Ω—ã –≤ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è –ª–µ–≥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.profit_targets = [3, 5, 10]  # %
        self.stop_losses = [2, 3, 5]  # %
        self.time_windows_hours = [6, 12, 24, 48]  # –ß–∞—Å–æ–≤

        self.success_criteria = {
            'profit_targets': self.profit_targets,
            'stop_losses': self.stop_losses,
            'time_windows': self.time_windows_hours,
        }

        # ML –º–æ–¥–µ–ª–∏
        self.models: Dict[str, Any] = {}
        self.scaler = StandardScaler()

    def _create_db_connection(self):
        """–°–æ–∑–¥–∞–µ—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
        try:
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–±—Ä–∞–Ω –¥—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è –∏–º–ø–æ—Ä—Ç
            user = os.getenv('DB_USER')
            password = quote_plus(os.getenv('DB_PASSWORD', ''))
            host = os.getenv('DB_HOST')
            port = os.getenv('DB_PORT', 3306)
            database = os.getenv('DB_NAME')

            if not all([user, password, host, database]):
                raise ValueError(
                    "–ù–µ –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –∑–∞–¥–∞–Ω—ã (DB_USER, DB_PASSWORD, DB_HOST, DB_NAME).")

            connection_string = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"
            engine = create_engine(connection_string, pool_pre_ping=True)

            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return engine
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
            raise

    def get_all_signals(self, limit: Optional[int] = None) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å–∏–≥–Ω–∞–ª—ã —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã.
        """
        print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö... (–õ–∏–º–∏—Ç: {limit or '–ù–µ—Ç'})")

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        limit_clause = "LIMIT :limit_val" if limit else ""

        query = text(f"""
        SELECT 
            s.id as signal_id, s.token_id, s.symbol, s.timestamp as signal_timestamp,
            s.OI_contracts_binance_now, s.OI_contracts_binance_prev, s.OI_contracts_binance_change,
            s.OI_contracts_bybit_now, s.OI_contracts_bybit_prev, s.OI_contracts_bybit_change,
            s.funding_rate_binance_now, s.funding_rate_binance_prev, s.funding_rate_binance_change,
            s.funding_rate_bybit_now, s.funding_rate_bybit_prev, s.funding_rate_bybit_change,
            s.volume_usd as signal_volume_usd, s.price_usd_now as signal_price,
            s.price_usd_prev as signal_price_prev, s.price_usd_change as price_change_10min,
            s.market_cap_usd,
            e.oi_usdt_average, e.oi_usdt_current, e.oi_usdt_yesterday,
            e.oi_usdt_change_current_to_yesterday, e.oi_usdt_change_current_to_average,
            e.spot_volume_usdt_average, e.spot_volume_usdt_current, e.spot_volume_usdt_yesterday,
            e.spot_volume_usdt_change_current_to_yesterday, e.spot_volume_usdt_change_current_to_average,
            e.spot_price_usdt_average, e.spot_price_usdt_current, e.spot_price_usdt_yesterday,
            e.spot_price_usdt_change_1h, e.spot_price_usdt_change_24h, e.spot_price_usdt_change_7d, e.spot_price_usdt_change_30d,
            e.cmc_price_min_1h, e.cmc_price_max_1h, e.cmc_price_min_24h, e.cmc_price_max_24h,
            e.cmc_price_min_7d, e.cmc_price_max_7d, e.cmc_price_min_30d, e.cmc_price_max_30d,
            e.cmc_percent_change_1d, e.cmc_percent_change_7d, e.cmc_percent_change_30d
        FROM signals_10min s
        LEFT JOIN signals_10min_enriched e ON s.id = e.signal_id
        WHERE s.OI_contracts_binance_change > 0
        ORDER BY s.timestamp DESC
        {limit_clause}
        """)

        params = {"limit_val": limit} if limit else {}
        df = pd.read_sql(query, self.engine, params=params)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–∏–≥–Ω–∞–ª–æ–≤")

        if not df.empty:
            df = self._add_technical_features(df)

        return df

    def _add_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏."""
        print("üîß –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –£–õ–£–ß–®–ï–ù–û: pd.to_datetime –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑
        df['signal_timestamp'] = pd.to_datetime(df['signal_timestamp'])
        df['hour'] = df['signal_timestamp'].dt.hour
        df['day_of_week'] = df['signal_timestamp'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

        df['oi_binance_bybit_ratio'] = df['OI_contracts_binance_change'] / (
                    df['OI_contracts_bybit_change'] + AVOID_ZERO_DIV_SMALL)
        df['funding_binance_bybit_ratio'] = df['funding_rate_binance_now'] / (
                    df['funding_rate_bybit_now'] + AVOID_ZERO_DIV_TINY)

        df['price_momentum'] = df['spot_price_usdt_change_1h'] * df['spot_price_usdt_change_24h']

        df['volatility_1h'] = ((df['cmc_price_max_1h'] - df['cmc_price_min_1h']) / df['signal_price'] * 100).fillna(0)
        df['volatility_24h'] = ((df['cmc_price_max_24h'] - df['cmc_price_min_24h']) / df['signal_price'] * 100).fillna(
            0)

        # –£–õ–£–ß–®–ï–ù–û: –≤–µ—Å–∞ –¥–ª—è —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞ –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã/–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        df['signal_strength'] = (
                df['OI_contracts_binance_change'] * 0.4 +
                df['OI_contracts_bybit_change'] * 0.3 +
                df['spot_volume_usdt_change_current_to_average'] * 0.2 +
                df['price_change_10min'] * 0.1
        ).fillna(0)

        return df

    def get_futures_data_for_signal(self, token_id: int, signal_time: datetime, hours_after: int = 48) -> pd.DataFrame:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Ñ—å—é—á–µ—Ä—Å–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞, –∏—Å–ø–æ–ª—å–∑—É—è –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å."""

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        query = text("""
                     SELECT fd.timestamp,
                            fd.price_usd,
                            fd.volume_usd,
                            fd.open_interest_usd
                     FROM futures_data fd
                              JOIN futures_pairs fp ON fd.pair_id = fp.id
                     WHERE fp.token_id = :token_id
                       AND fd.timestamp >= :start_time
                       AND fd.timestamp <= :end_time
                     ORDER BY fd.timestamp
                     """)

        params = {
            "token_id": token_id,
            "start_time": signal_time,
            "end_time": signal_time + timedelta(hours=hours_after)
        }

        return pd.read_sql(query, self.engine, params=params)

    def calculate_signal_outcomes_batch(self, signals_df: pd.DataFrame, batch_size: int = 100) -> pd.DataFrame:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –±–∞—Ç—á–∞–º–∏."""
        print(f"\nüîÑ –†–∞—Å—á–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è {len(signals_df)} —Å–∏–≥–Ω–∞–ª–æ–≤...")
        all_results = []

        max_hours = max(self.time_windows_hours)

        for _, signal in tqdm(signals_df.iterrows(), total=len(signals_df)):
            try:
                futures_data = self.get_futures_data_for_signal(
                    signal['token_id'],
                    signal['signal_timestamp'],
                    hours_after=max_hours
                )

                if not futures_data.empty:
                    outcomes = self._calculate_multi_criteria_outcomes(signal, futures_data)
                    signal_dict = signal.to_dict()
                    signal_dict.update(outcomes)
                    all_results.append(signal_dict)

            except Exception as e:
                # –£–õ–£–ß–®–ï–ù–û: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –æ—à–∏–±–∫–∏ –¥–ª—è –ª—É—á—à–µ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–∏–≥–Ω–∞–ª–∞ {signal['signal_id']} ({signal['symbol']}): {e}")
                continue

        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(all_results)} —Å–∏–≥–Ω–∞–ª–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
        return pd.DataFrame(all_results)

    def _calculate_multi_criteria_outcomes(self, signal: pd.Series, futures_data: pd.DataFrame) -> Dict[str, Any]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –ø–æ —Ä–∞–∑–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º."""
        entry_price = signal['signal_price']
        outcomes: Dict[str, Any] = {}

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ–±–∞ —Å—Ç–æ–ª–±—Ü–∞ –∏–º–µ—é—Ç —Ç–∏–ø datetime
        futures_data['timestamp'] = pd.to_datetime(futures_data['timestamp'])
        signal_time = pd.to_datetime(signal['signal_timestamp'])

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ä–∞–∑–Ω–∏—Ü—ã –≤–æ –≤—Ä–µ–º–µ–Ω–∏
        futures_data['time_diff_min'] = (futures_data['timestamp'] - signal_time).dt.total_seconds() / 60
        futures_data['pnl_pct'] = ((futures_data['price_usd'] - entry_price) / entry_price) * 100

        outcomes['max_profit_pct'] = futures_data['pnl_pct'].max()
        outcomes['max_drawdown_pct'] = futures_data['pnl_pct'].min()
        outcomes['final_pnl_pct'] = futures_data['pnl_pct'].iloc[-1] if not futures_data.empty else 0

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏/—É–±—ã—Ç–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º –∏–Ω–¥–µ–∫—Å–∞
        if outcomes['max_profit_pct'] > 0:
            outcomes['time_to_max_profit_min'] = futures_data.loc[futures_data['pnl_pct'].idxmax(), 'time_diff_min']
        else:
            outcomes['time_to_max_profit_min'] = None

        if outcomes['max_drawdown_pct'] < 0:
            outcomes['time_to_max_drawdown_min'] = futures_data.loc[futures_data['pnl_pct'].idxmin(), 'time_diff_min']
        else:
            outcomes['time_to_max_drawdown_min'] = None

        for profit_target in self.profit_targets:
            for stop_loss in self.stop_losses:
                for time_window in self.time_windows_hours:
                    success_key = f'success_{profit_target}p_{stop_loss}sl_{time_window}h'
                    window_data = futures_data[futures_data['time_diff_min'] <= time_window * 60]

                    if window_data.empty:
                        outcomes[success_key] = 0
                        continue

                    profit_times = window_data.index[window_data['pnl_pct'] >= profit_target]
                    stop_times = window_data.index[window_data['pnl_pct'] <= -stop_loss]

                    first_profit_time = profit_times.min() if not profit_times.empty else np.inf
                    first_stop_time = stop_times.min() if not stop_times.empty else np.inf

                    is_successful = first_profit_time < first_stop_time
                    outcomes[success_key] = int(is_successful)

                    # –£–õ–£–ß–®–ï–ù–û: –ó–∞–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–π)
                    if profit_target == 5 and stop_loss == 3 and time_window == 24:
                        outcomes['is_successful_main'] = int(is_successful)
                        outcomes['hit_profit_target'] = int(not profit_times.empty)
                        outcomes['hit_stop_loss'] = int(not stop_times.empty)

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        if len(futures_data) > 1:
            outcomes['volatility_after'] = futures_data['pnl_pct'].std()
            outcomes['volume_trend'] = 1 if futures_data['volume_usd'].iloc[-1] > futures_data['volume_usd'].iloc[
                0] else -1  # 1: increasing, -1: decreasing

            oi_initial = futures_data['open_interest_usd'].iloc[0]
            if oi_initial > 0:
                outcomes['oi_change_after'] = ((futures_data['open_interest_usd'].iloc[
                                                    -1] - oi_initial) / oi_initial) * 100
            else:
                outcomes['oi_change_after'] = 0
        else:
            outcomes['volatility_after'] = 0
            outcomes['volume_trend'] = 0  # neutral
            outcomes['oi_change_after'] = 0

        return outcomes

    def prepare_ml_features(self, df: pd.DataFrame, target_column: str = 'is_successful_main') -> Tuple[
        pd.DataFrame, pd.Series, List[str]]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ (X) –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (y) –¥–ª—è ML –º–æ–¥–µ–ª–µ–π.
        """
        print("\nü§ñ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML...")

        feature_columns = [
            'OI_contracts_binance_change', 'OI_contracts_bybit_change', 'OI_contracts_binance_now',
            'OI_contracts_bybit_now',
            'funding_rate_binance_now', 'funding_rate_bybit_now', 'funding_rate_binance_change',
            'funding_rate_bybit_change',
            'price_change_10min', 'spot_price_usdt_change_1h', 'spot_price_usdt_change_24h',
            'spot_price_usdt_change_7d',
            'signal_volume_usd', 'spot_volume_usdt_current', 'spot_volume_usdt_change_current_to_average',
            'spot_volume_usdt_change_current_to_yesterday',
            'oi_usdt_current', 'oi_usdt_change_current_to_average', 'oi_usdt_change_current_to_yesterday',
            'hour', 'day_of_week', 'is_weekend', 'oi_binance_bybit_ratio', 'funding_binance_bybit_ratio',
            'price_momentum', 'volatility_1h', 'volatility_24h', 'signal_strength',
            'cmc_percent_change_1d', 'cmc_percent_change_7d', 'cmc_percent_change_30d'
        ]

        available_features = [col for col in feature_columns if col in df.columns]

        df_clean = df.dropna(subset=[target_column])

        # –£–õ–£–ß–®–ï–ù–û: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN –º–µ–¥–∏–∞–Ω–æ–π –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª–µ–µ —Ä–æ–±–∞—Å—Ç–Ω—ã–º, —á–µ–º –Ω—É–ª–µ–º
        X = df_clean[available_features].fillna(df_clean[available_features].median())
        y = df_clean[target_column]

        print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ —Å {len(available_features)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        if not y.empty:
            class_distribution = y.value_counts(normalize=True)
            print(
                f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: 0: {class_distribution.get(0, 0):.2%}, 1: {class_distribution.get(1, 0):.2%}")

        return X, y, available_features

    def train_ml_models(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """–û–±—É—á–∞–µ—Ç, –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 4 ML –º–æ–¥–µ–ª–∏."""
        print("\nüéØ –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π...")
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        results = {}

        # --- 1. Random Forest ---
        print("\n1Ô∏è‚É£  Random Forest...")
        rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=20, class_weight='balanced',
                                          random_state=42, n_jobs=-1)
        rf_model.fit(X_train, y_train)
        results['random_forest'] = self._evaluate_model(rf_model, X_test, y_test, X.columns)

        # --- 2. XGBoost ---
        print("2Ô∏è‚É£  XGBoost...")
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum() if (y_train == 1).sum() > 0 else 1

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: 'eval_metric' —É–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ, –∞ –Ω–µ –≤ .fit()
        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=scale_pos_weight,
            random_state=42,
            eval_metric='logloss'  # <-- –ü–∞—Ä–∞–º–µ—Ç—Ä —Ç–µ–ø–µ—Ä—å –∑–¥–µ—Å—å
        )

        # –£–õ–£–ß–®–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω eval_set –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ early stopping
        eval_set = [(X_test, y_test)]
        xgb_model.fit(X_train, y_train, eval_set=eval_set, verbose=False)

        results['xgboost'] = self._evaluate_model(xgb_model, X_test, y_test)

        # --- 3. LightGBM ---
        print("3Ô∏è‚É£  LightGBM...")
        lgb_model = lgb.LGBMClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, class_weight='balanced',
                                       random_state=42, verbosity=-1)
        lgb_model.fit(X_train, y_train)
        results['lightgbm'] = self._evaluate_model(lgb_model, X_test, y_test)

        # --- 4. Neural Network ---
        print("4Ô∏è‚É£  Neural Network...")
        nn_model = MLPClassifier(hidden_layer_sizes=(128, 64, 32), activation='relu', solver='adam', alpha=0.001,
                                 learning_rate='adaptive', max_iter=500, early_stopping=True, random_state=42)
        nn_model.fit(X_train_scaled, y_train)
        results['neural_network'] = self._evaluate_model(nn_model, X_test_scaled, y_test)

        self.models = {name: res['model'] for name, res in results.items()}
        self._print_training_results(results)
        return results

    def analyze_feature_performance(self, df: pd.DataFrame) -> List[str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ —Ä–∞–∑—Ä–µ–∑–µ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,
        —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –∏ –Ω–∞–∏–º–µ–Ω–µ–µ —É—Å–ø–µ—à–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        """
        if 'is_successful_main' not in df.columns or df['is_successful_main'].isna().all():
            return []

        report_lines = ["", "=" * 80, "–î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í", "=" * 80]

        # --- –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏ ---
        day_map = {0: '–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', 1: '–í—Ç–æ—Ä–Ω–∏–∫', 2: '–°—Ä–µ–¥–∞', 3: '–ß–µ—Ç–≤–µ—Ä–≥', 4: '–ü—è—Ç–Ω–∏—Ü–∞', 5: '–°—É–±–±–æ—Ç–∞',
                   6: '–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ'}
        if 'day_of_week' in df.columns:
            report_lines.extend(["", "–ê–ù–ê–õ–ò–ó –ü–û –î–ù–Ø–ú –ù–ï–î–ï–õ–ò:"])
            daily_stats = df.groupby('day_of_week')['is_successful_main'].agg(['mean', 'count']).rename(index=day_map)
            daily_stats = daily_stats.sort_values(by='mean', ascending=False)
            for day, stats in daily_stats.iterrows():
                report_lines.append(f"  - {day:<12}: {stats['mean']:.1%} —É—Å–ø–µ—Ö–∞ ({int(stats['count'])} —Å–∏–≥–Ω–∞–ª–æ–≤)")

        # --- –ê–Ω–∞–ª–∏–∑ –ø–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ (24—á) ---
        if 'volatility_24h' in df.columns:
            report_lines.extend(["", "–ê–ù–ê–õ–ò–ó –ü–û 24–ß –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–ò:"])
            vol_bins = [-np.inf, 3, 6, 10, 15, np.inf]
            vol_labels = ["–ù–∏–∑–∫–∞—è (<3%)", "–°—Ä–µ–¥–Ω—è—è (3-6%)", "–í—ã—Å–æ–∫–∞—è (6-10%)", "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è (10-15%)",
                          "–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞—è (>15%)"]
            df['volatility_bin'] = pd.cut(df['volatility_24h'], bins=vol_bins, labels=vol_labels, right=False)
            vol_stats = df.groupby('volatility_bin')['is_successful_main'].agg(['mean', 'count'])
            vol_stats = vol_stats.sort_values(by='mean', ascending=False)
            for vol_range, stats in vol_stats.iterrows():
                report_lines.append(f"  - {vol_range:<25}: {stats['mean']:.1%} —É—Å–ø–µ—Ö–∞ ({int(stats['count'])} —Å–∏–≥–Ω–∞–ª–æ–≤)")

        # --- –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç—Ä–µ–Ω–¥—É –∑–∞ 7 –¥–Ω–µ–π ---
        if 'cmc_percent_change_7d' in df.columns:
            report_lines.extend(["", "–ê–ù–ê–õ–ò–ó –ü–û 7–î –¢–†–ï–ù–î–£ –¶–ï–ù–´:"])
            trend_bins = [-np.inf, -20, -5, 5, 20, np.inf]
            trend_labels = ["–°–∏–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ (<-20%)", "–ü–∞–¥–µ–Ω–∏–µ (-20%...-5%)", "–ë–æ–∫–æ–≤–∏–∫ (-5%...+5%)",
                            "–†–æ—Å—Ç (+5%...+20%)", "–°–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç (>+20%)"]
            df['trend_bin_7d'] = pd.cut(df['cmc_percent_change_7d'], bins=trend_bins, labels=trend_labels, right=False)
            trend_stats = df.groupby('trend_bin_7d')['is_successful_main'].agg(['mean', 'count'])
            trend_stats = trend_stats.sort_values(by='mean', ascending=False)
            for trend_range, stats in trend_stats.iterrows():
                report_lines.append(
                    f"  - {trend_range:<25}: {stats['mean']:.1%} —É—Å–ø–µ—Ö–∞ ({int(stats['count'])} —Å–∏–≥–Ω–∞–ª–æ–≤)")

        # --- –ê–Ω–∞–ª–∏–∑ –ø–æ —Ä–µ–∞–∫—Ü–∏–∏ —Ü–µ–Ω—ã –∑–∞ 10 –º–∏–Ω—É—Ç ---
        if 'price_change_10min' in df.columns:
            report_lines.extend(["", "–ê–ù–ê–õ–ò–ó –ü–û –†–ï–ê–ö–¶–ò–ò –¶–ï–ù–´ (10 –ú–ò–ù):"])
            react_bins = [-np.inf, -1, -0.25, 0.25, 1, np.inf]
            react_labels = ["–ü–∞–¥–µ–Ω–∏–µ (<-1%)", "–°–Ω–∏–∂–µ–Ω–∏–µ (-1%...-0.25%)", "–ù–∞ –º–µ—Å—Ç–µ (-0.25%...+0.25%)",
                            "–†–æ—Å—Ç (+0.25%...+1%)", "–°–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç (>+1%)"]
            df['react_bin_10m'] = pd.cut(df['price_change_10min'], bins=react_bins, labels=react_labels, right=False)
            react_stats = df.groupby('react_bin_10m')['is_successful_main'].agg(['mean', 'count'])
            react_stats = react_stats.sort_values(by='mean', ascending=False)
            for react_range, stats in react_stats.iterrows():
                report_lines.append(
                    f"  - {react_range:<28}: {stats['mean']:.1%} —É—Å–ø–µ—Ö–∞ ({int(stats['count'])} —Å–∏–≥–Ω–∞–ª–æ–≤)")

        report_lines.append("=" * 80)
        return report_lines

    def _evaluate_model(self, model: Any, X_test: pd.DataFrame, y_test: pd.Series,
                        feature_names: Optional[pd.Index] = None) -> Dict:
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏."""
        pred = model.predict(X_test)
        pred_proba = model.predict_proba(X_test)[:, 1]

        report = classification_report(y_test, pred, output_dict=True, zero_division=0)

        metrics = {
            'model': model,
            'accuracy': report['accuracy'],
            'auc': roc_auc_score(y_test, pred_proba),
            'report': report,
        }

        if feature_names is not None and hasattr(model, 'feature_importances_'):
            metrics['feature_importance'] = pd.DataFrame({
                'feature': feature_names,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)

        return metrics

    def _print_training_results(self, results: Dict):
        """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Å–æ–ª—å."""
        print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è:")
        print("-" * 60)
        for model_name, metrics in results.items():
            print(f"\n{model_name.upper()}:")
            print(f"  Accuracy: {metrics['accuracy']:.4f}")
            print(f"  AUC: {metrics['auc']:.4f}")
            print(f"  Precision (class 1): {metrics['report']['1']['precision']:.3f}")
            print(f"  Recall (class 1): {metrics['report']['1']['recall']:.3f}")

    def save_models_and_data(self, feature_names: List[str], results: Dict, output_dir: str = 'models'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Å–∫–µ–π–ª–µ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ."""
        os.makedirs(output_dir, exist_ok=True)

        for model_name, model in self.models.items():
            joblib.dump(model, os.path.join(output_dir, f'{model_name}_model.pkl'))

        joblib.dump(self.scaler, os.path.join(output_dir, 'scaler.pkl'))

        with open(os.path.join(output_dir, 'feature_names.json'), 'w') as f:
            json.dump(feature_names, f)

        json_results = {
            model_name: {
                'accuracy': float(m['accuracy']), 'auc': float(m['auc']),
                'precision_1': float(m['report']['1']['precision']), 'recall_1': float(m['report']['1']['recall'])
            } for model_name, m in results.items()
        }
        with open(os.path.join(output_dir, 'training_results.json'), 'w') as f:
            json.dump(json_results, f, indent=2)

        if 'random_forest' in results and 'feature_importance' in results['random_forest']:
            results['random_forest']['feature_importance'].to_csv(
                os.path.join(output_dir, 'feature_importance.csv'), index=False
            )

        print(f"\nüíæ –ú–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ '{output_dir}/'")

    def generate_comprehensive_report(self, df: pd.DataFrame, results: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç."""
        print("\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–∞ –ª–æ–≥–∏–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        report_lines = [
            "=" * 80,
            "–ü–û–õ–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –ö–†–ò–ü–¢–û–í–ê–õ–Æ–¢–ù–´–• –°–ò–ì–ù–ê–õ–û–í",
            f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 80, "",
            "–û–ë–†–ê–ë–û–¢–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï:",
            f"- –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(df)}",
            f"- –ü–µ—Ä–∏–æ–¥: {df['signal_timestamp'].min()} - {df['signal_timestamp'].max()}",
            f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {df['symbol'].nunique()}", "",
            "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –†–ê–ó–ù–´–ú –ö–†–ò–¢–ï–†–ò–Ø–ú –£–°–ü–ï–•–ê:",
        ]

        for profit in self.profit_targets:
            for stop in self.stop_losses:
                for window in self.time_windows_hours:
                    col_name = f'success_{profit}p_{stop}sl_{window}h'
                    if col_name in df.columns:
                        success_rate = df[col_name].mean() * 100
                        count = df[col_name].sum()
                        report_lines.append(
                            f"- {profit}% –ø. / {stop}% —É. / {window}—á: {success_rate:.1f}% —É—Å–ø–µ—Ö–∞ ({count} –∏–∑ {len(df)})"
                        )

        report_lines.extend(["", "–¢–û–ü-10 –¢–û–ö–ï–ù–û–í –ü–û –ö–û–õ–ò–ß–ï–°–¢–í–£ –°–ò–ì–ù–ê–õ–û–í:"])
        top_tokens = df['symbol'].value_counts().head(10)
        for i, (token, count) in enumerate(top_tokens.items(), 1):
            token_df = df[df['symbol'] == token]
            if 'is_successful_main' in token_df.columns and not token_df.empty:
                success_rate = token_df['is_successful_main'].mean() * 100
                report_lines.append(f"{i}. {token}: {count} —Å–∏–≥–Ω–∞–ª–æ–≤ ({success_rate:.1f}% —É—Å–ø–µ—Ö–∞)")
            else:
                report_lines.append(f"{i}. {token}: {count} —Å–∏–≥–Ω–∞–ª–æ–≤")

        report_lines.extend(["", "–†–ï–ó–£–õ–¨–¢–ê–¢–´ ML –ú–û–î–ï–õ–ï–ô (–¶–µ–ª—å: 5%–ø/3%—É/24—á):"])
        for model_name, metrics in results.items():
            report_lines.extend([
                f"\n{model_name.upper()}:",
                f"  - Accuracy: {metrics['accuracy']:.4f}", f"  - AUC: {metrics['auc']:.4f}",
                f"  - Precision: {metrics['report']['1']['precision']:.3f}",
                f"  - Recall: {metrics['report']['1']['recall']:.3f}"
            ])

        if 'random_forest' in results and 'feature_importance' in results['random_forest']:
            report_lines.extend(["", "–¢–û–ü-15 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í (Random Forest):"])
            top_features = results['random_forest']['feature_importance'].head(15)
            for idx, row in top_features.iterrows():
                report_lines.append(f"  {idx + 1}. {row['feature']}: {row['importance']:.4f}")

        if 'hour' in df.columns and 'is_successful_main' in df.columns:
            report_lines.extend(["", "–í–†–ï–ú–ï–ù–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´:"])
            hourly_success = df.groupby('hour')['is_successful_main'].agg(['mean', 'count'])
            best_hours = hourly_success.sort_values('mean', ascending=False).head(5)
            report_lines.append("–õ—É—á—à–∏–µ —á–∞—Å—ã –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤ (UTC):")
            for hour, stats in best_hours.iterrows():
                report_lines.append(
                    f"  - {hour:02d}:00 - {stats['mean'] * 100:.1f}% —É—Å–ø–µ—Ö–∞ ({stats['count']} —Å–∏–≥–Ω–∞–ª–æ–≤)")

        feature_analysis_lines = self.analyze_feature_performance(
            df.copy())  # .copy() —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å SettingWithCopyWarning
        report_lines.extend(feature_analysis_lines)

        os.makedirs('reports', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = f'reports/full_analysis_report_{timestamp}.txt'

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

        print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        print('\n' + '\n'.join(report_lines[:20]) + '\n...')
        return report_path


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–∏–≥–Ω–∞–ª–æ–≤."""
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å–∏–≥–Ω–∞–ª–æ–≤...")
    print("=" * 80)

    try:
        processor = FullSignalsProcessor()

        limit_signals = 1000  # None –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö, —á–∏—Å–ª–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

        signals = processor.get_all_signals(limit=limit_signals)

        if signals.empty:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            return

        signals_with_outcomes = processor.calculate_signal_outcomes_batch(signals, batch_size=100)

        if signals_with_outcomes.empty:
            print("‚ùå –ü–æ—Å–ª–µ —Ä–∞—Å—á–µ—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            return

        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        signals_with_outcomes.to_pickle('all_signals_with_outcomes.pkl')
        signals_with_outcomes.to_csv('all_signals_with_outcomes.csv', index=False)

        X, y, feature_names = processor.prepare_ml_features(signals_with_outcomes)

        if X.empty or y.empty:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            return

        ml_results = processor.train_ml_models(X, y)

        processor.save_models_and_data(feature_names, ml_results)

        report_path = processor.generate_comprehensive_report(signals_with_outcomes, ml_results)

        print("\n" + "=" * 60)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print("=" * 60)
        best_model_name, best_model_metrics = max(ml_results.items(), key=lambda item: item[1]['auc'])
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   - –î–∞–Ω–Ω—ã–µ: all_signals_with_outcomes.pkl/csv")
        print(f"   - –ú–æ–¥–µ–ª–∏: models/")
        print(f"   - –û—Ç—á–µ—Ç: {report_path}")
        print(f"üèÜ –õ—É—á—à–∞—è ML –º–æ–¥–µ–ª—å: {best_model_name.upper()} (AUC: {best_model_metrics['auc']:.4f})")

    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –í –û–°–ù–û–í–ù–û–ú –ü–†–û–¶–ï–°–°–ï: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()