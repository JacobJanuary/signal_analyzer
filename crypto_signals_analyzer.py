#!/usr/bin/env python3
"""
Crypto Signals Analyzer - –ü–æ–ª–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
"""

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Tuple, Optional
from dotenv import load_dotenv
import warnings

warnings.filterwarnings('ignore')

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


class CryptoSignalsAnalyzer:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
    """

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        self.engine = self._create_db_connection()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞
        self.signal_threshold = {
            'oi_change_min': 3.0,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç OI –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞
            'funding_positive': True,  # –¢—Ä–µ–±–æ–≤–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–Ω–¥–∏–Ω–≥
            'hours_to_analyze': 24,  # –ß–∞—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ —Å–∏–≥–Ω–∞–ª–∞
            'profit_target': 5.0,  # –¶–µ–ª–µ–≤–∞—è –ø—Ä–∏–±—ã–ª—å %
            'stop_loss': 3.0  # –°—Ç–æ–ø-–ª–æ—Å—Å %
        }

    def _create_db_connection(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            engine = create_engine(
                f"mysql+pymysql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@"
                f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT', 3306)}/{os.getenv('DB_NAME')}",
                pool_pre_ping=True
            )
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return engine
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
            raise

    def get_recent_signals(self, days: int = 30, limit: int = 1000) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–∏–≥–Ω–∞–ª—ã –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥

        Args:
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤
        """
        query = f"""
        SELECT 
            s.id as signal_id,
            s.token_id,
            s.symbol,
            s.timestamp as signal_timestamp,
            s.OI_contracts_binance_change,
            s.OI_contracts_bybit_change,
            s.funding_rate_binance_now,
            s.funding_rate_bybit_now,
            s.volume_usd as signal_volume_usd,
            s.price_usd_now as signal_price,
            s.price_usd_change as price_change_10min,
            s.market_cap_usd,

            -- –î–∞–Ω–Ω—ã–µ –∏–∑ enriched —Ç–∞–±–ª–∏—Ü—ã
            e.oi_usdt_change_current_to_average,
            e.oi_usdt_change_current_to_yesterday,
            e.spot_volume_usdt_change_current_to_average,
            e.spot_volume_usdt_change_current_to_yesterday,
            e.spot_price_usdt_change_1h,
            e.spot_price_usdt_change_24h,
            e.spot_price_usdt_change_7d,
            e.spot_price_usdt_change_30d,
            e.cmc_price_min_24h,
            e.cmc_price_max_24h,
            e.cmc_price_min_7d,
            e.cmc_price_max_7d

        FROM signals_10min s
        LEFT JOIN signals_10min_enriched e ON s.id = e.signal_id
        WHERE s.timestamp >= DATE_SUB(NOW(), INTERVAL {days} DAY)
            AND s.OI_contracts_binance_change >= {self.signal_threshold['oi_change_min']}
        ORDER BY s.timestamp DESC
        LIMIT {limit}
        """

        print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days} –¥–Ω–µ–π...")
        df = pd.read_sql(query, self.engine)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–∏–≥–Ω–∞–ª–æ–≤")

        return df

    def calculate_signal_outcomes(self, signals_df: pd.DataFrame) -> pd.DataFrame:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
        """
        print("\nüîÑ –†–∞—Å—á–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤...")

        results = []
        total_signals = len(signals_df)

        for idx, signal in signals_df.iterrows():
            if idx % 50 == 0:
                print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx}/{total_signals} —Å–∏–≥–Ω–∞–ª–æ–≤...", end='\r')

            outcome = self._analyze_signal_outcome(
                signal['token_id'],
                signal['signal_timestamp'],
                signal['signal_price']
            )

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
            signal_dict = signal.to_dict()
            signal_dict.update(outcome)
            results.append(signal_dict)

        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_signals} —Å–∏–≥–Ω–∞–ª–æ–≤")
        return pd.DataFrame(results)

    def _analyze_signal_outcome(self, token_id: int, signal_time: datetime,
                                entry_price: float) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
        """
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ —Å–∏–≥–Ω–∞–ª–∞
        query = f"""
        SELECT 
            fd.timestamp,
            fd.price_usd,
            fd.open_interest_usd,
            fd.volume_usd
        FROM futures_data fd
        JOIN futures_pairs fp ON fd.pair_id = fp.id
        WHERE fp.token_id = {token_id}
            AND fd.timestamp > '{signal_time}'
            AND fd.timestamp <= '{signal_time + timedelta(hours=self.signal_threshold['hours_to_analyze'])}'
        ORDER BY fd.timestamp
        LIMIT 500
        """

        try:
            price_data = pd.read_sql(query, self.engine)

            if price_data.empty or entry_price == 0:
                return self._empty_outcome()

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º P&L
            price_data['pnl_pct'] = ((price_data['price_usd'] - entry_price) / entry_price) * 100

            # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            max_profit = price_data['pnl_pct'].max()
            max_drawdown = price_data['pnl_pct'].min()

            # –í—Ä–µ–º—è –¥–æ —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
            time_to_max_profit = None
            time_to_max_drawdown = None

            if max_profit > 0:
                max_profit_idx = price_data['pnl_pct'].idxmax()
                time_to_max_profit = (price_data.loc[max_profit_idx, 'timestamp'] - signal_time).total_seconds() / 60

            if max_drawdown < 0:
                max_drawdown_idx = price_data['pnl_pct'].idxmin()
                time_to_max_drawdown = (price_data.loc[
                                            max_drawdown_idx, 'timestamp'] - signal_time).total_seconds() / 60

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π
            hit_profit_target = (price_data['pnl_pct'] >= self.signal_threshold['profit_target']).any()
            hit_stop_loss = (price_data['pnl_pct'] <= -self.signal_threshold['stop_loss']).any()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
            if hit_profit_target and hit_stop_loss:
                # –ß—Ç–æ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ?
                first_target_idx = price_data[price_data['pnl_pct'] >= self.signal_threshold['profit_target']].index[0]
                first_stop_idx = price_data[price_data['pnl_pct'] <= -self.signal_threshold['stop_loss']].index[0]
                is_successful = first_target_idx < first_stop_idx
            else:
                is_successful = hit_profit_target

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final_pnl = price_data.iloc[-1]['pnl_pct'] if not price_data.empty else 0

            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            volatility = price_data['pnl_pct'].std()

            # –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤
            avg_volume = price_data['volume_usd'].mean()
            volume_trend = 'increasing' if price_data['volume_usd'].iloc[-1] > price_data['volume_usd'].iloc[
                0] else 'decreasing'

            return {
                'max_profit_pct': round(max_profit, 2),
                'max_drawdown_pct': round(max_drawdown, 2),
                'time_to_max_profit_min': round(time_to_max_profit, 0) if time_to_max_profit else None,
                'time_to_max_drawdown_min': round(time_to_max_drawdown, 0) if time_to_max_drawdown else None,
                'final_pnl_pct': round(final_pnl, 2),
                'is_successful': int(is_successful),
                'hit_profit_target': int(hit_profit_target),
                'hit_stop_loss': int(hit_stop_loss),
                'volatility': round(volatility, 2),
                'avg_volume_after': avg_volume,
                'volume_trend': volume_trend,
                'data_points': len(price_data)
            }

        except Exception as e:
            print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–∏–≥–Ω–∞–ª–∞: {e}")
            return self._empty_outcome()

    def _empty_outcome(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        return {
            'max_profit_pct': 0,
            'max_drawdown_pct': 0,
            'time_to_max_profit_min': None,
            'time_to_max_drawdown_min': None,
            'final_pnl_pct': 0,
            'is_successful': 0,
            'hit_profit_target': 0,
            'hit_stop_loss': 0,
            'volatility': 0,
            'avg_volume_after': 0,
            'volume_trend': 'unknown',
            'data_points': 0
        }

    def analyze_patterns(self, df: pd.DataFrame) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã —É—Å–ø–µ—à–Ω—ã—Ö –∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
        """
        print("\nüìà –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —É—Å–ø–µ—à–Ω—ã–µ –∏ –Ω–µ—É—Å–ø–µ—à–Ω—ã–µ
        successful = df[df['is_successful'] == 1]
        failed = df[df['is_successful'] == 0]

        analysis = {
            'general_stats': {
                'total_signals': len(df),
                'successful_signals': len(successful),
                'failed_signals': len(failed),
                'success_rate': len(successful) / len(df) * 100 if len(df) > 0 else 0,
                'avg_max_profit': successful['max_profit_pct'].mean() if len(successful) > 0 else 0,
                'avg_max_loss': failed['max_drawdown_pct'].mean() if len(failed) > 0 else 0,
                'avg_time_to_profit': successful['time_to_max_profit_min'].mean() if len(successful) > 0 else 0
            },
            'successful_patterns': self._analyze_group_patterns(successful),
            'failed_patterns': self._analyze_group_patterns(failed),
            'best_tokens': self._find_best_tokens(df),
            'best_time_slots': self._find_best_time_slots(df),
            'correlation_analysis': self._analyze_correlations(df)
        }

        return analysis

    def _analyze_group_patterns(self, group_df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≥—Ä—É–ø–ø—ã —Å–∏–≥–Ω–∞–ª–æ–≤"""
        if len(group_df) == 0:
            return {}

        return {
            'avg_oi_change_binance': group_df['OI_contracts_binance_change'].mean(),
            'avg_oi_change_bybit': group_df['OI_contracts_bybit_change'].mean(),
            'avg_funding_binance': group_df['funding_rate_binance_now'].mean(),
            'avg_funding_bybit': group_df['funding_rate_bybit_now'].mean(),
            'avg_volume_change': group_df[
                'spot_volume_usdt_change_current_to_average'].mean() if 'spot_volume_usdt_change_current_to_average' in group_df else 0,
            'avg_price_change_1h': group_df[
                'spot_price_usdt_change_1h'].mean() if 'spot_price_usdt_change_1h' in group_df else 0,
            'avg_price_change_24h': group_df[
                'spot_price_usdt_change_24h'].mean() if 'spot_price_usdt_change_24h' in group_df else 0,
            'count': len(group_df)
        }

    def _find_best_tokens(self, df: pd.DataFrame, top_n: int = 10) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —É—Å–ø–µ—à–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã"""
        token_stats = df.groupby('symbol').agg({
            'is_successful': ['sum', 'count', 'mean'],
            'max_profit_pct': 'mean',
            'max_drawdown_pct': 'mean'
        }).round(2)

        token_stats.columns = ['successful_signals', 'total_signals', 'success_rate', 'avg_max_profit', 'avg_max_loss']
        token_stats['success_rate'] = token_stats['success_rate'] * 100

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã —Å –º–∏–Ω–∏–º—É–º 5 —Å–∏–≥–Ω–∞–ª–∞–º–∏
        token_stats = token_stats[token_stats['total_signals'] >= 5]

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        token_stats = token_stats.sort_values('success_rate', ascending=False).head(top_n)

        return token_stats.reset_index().to_dict('records')

    def _find_best_time_slots(self, df: pd.DataFrame) -> Dict:
        """–ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–µ–µ –≤—Ä–µ–º—è –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏"""
        df['hour'] = pd.to_datetime(df['signal_timestamp']).dt.hour
        df['day_of_week'] = pd.to_datetime(df['signal_timestamp']).dt.dayofweek

        hourly_stats = df.groupby('hour')['is_successful'].agg(['sum', 'count', 'mean'])
        hourly_stats['success_rate'] = hourly_stats['mean'] * 100

        best_hours = hourly_stats.sort_values('success_rate', ascending=False).head(5)
        worst_hours = hourly_stats.sort_values('success_rate', ascending=True).head(5)

        return {
            'best_hours': best_hours.index.tolist(),
            'worst_hours': worst_hours.index.tolist(),
            'hourly_success_rates': hourly_stats['success_rate'].to_dict()
        }

    def _analyze_correlations(self, df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —É—Å–ø–µ—à–Ω–æ—Å—Ç—å—é"""
        numeric_columns = [
            'OI_contracts_binance_change', 'OI_contracts_bybit_change',
            'funding_rate_binance_now', 'funding_rate_bybit_now',
            'spot_volume_usdt_change_current_to_average',
            'spot_price_usdt_change_1h', 'spot_price_usdt_change_24h'
        ]

        correlations = {}
        for col in numeric_columns:
            if col in df.columns:
                corr = df[col].corr(df['is_successful'])
                if not np.isnan(corr):
                    correlations[col] = round(corr, 3)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é
        sorted_corr = dict(sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True))

        return sorted_corr

    def generate_report(self, analysis: Dict, output_path: str = 'reports'):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        os.makedirs(output_path, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –æ—Ç—á–µ—Ç
        with open(f'{output_path}/analysis_report_{timestamp}.json', 'w') as f:
            json.dump(analysis, f, indent=2, default=str)

        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        report_lines = [
            "=" * 80,
            "–û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –ö–†–ò–ü–¢–û–í–ê–õ–Æ–¢–ù–´–• –°–ò–ì–ù–ê–õ–û–í",
            f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 80,
            "",
            "–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:",
            f"- –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {analysis['general_stats']['total_signals']}",
            f"- –£—Å–ø–µ—à–Ω—ã—Ö: {analysis['general_stats']['successful_signals']}",
            f"- –ù–µ—É—Å–ø–µ—à–Ω—ã—Ö: {analysis['general_stats']['failed_signals']}",
            f"- –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {analysis['general_stats']['success_rate']:.1f}%",
            f"- –°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å. –ø—Ä–∏–±—ã–ª—å: {analysis['general_stats']['avg_max_profit']:.2f}%",
            f"- –°—Ä–µ–¥–Ω–∏–π –º–∞–∫—Å. —É–±—ã—Ç–æ–∫: {analysis['general_stats']['avg_max_loss']:.2f}%",
            f"- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –¥–æ –º–∞–∫—Å. –ø—Ä–∏–±—ã–ª–∏: {analysis['general_stats']['avg_time_to_profit']:.0f} –º–∏–Ω",
            "",
            "–ü–ê–¢–¢–ï–†–ù–´ –£–°–ü–ï–®–ù–´–• –°–ò–ì–ù–ê–õ–û–í:",
            f"- –°—Ä–µ–¥–Ω–∏–π —Ä–æ—Å—Ç OI (Binance): {analysis['successful_patterns'].get('avg_oi_change_binance', 0):.2f}%",
            f"- –°—Ä–µ–¥–Ω–∏–π —Ä–æ—Å—Ç OI (Bybit): {analysis['successful_patterns'].get('avg_oi_change_bybit', 0):.2f}%",
            f"- –°—Ä–µ–¥–Ω–∏–π funding (Binance): {analysis['successful_patterns'].get('avg_funding_binance', 0):.4f}%",
            f"- –°—Ä–µ–¥–Ω–µ–µ –∏–∑–º. –æ–±—ä–µ–º–∞: {analysis['successful_patterns'].get('avg_volume_change', 0):.2f}%",
            "",
            "–¢–û–ü-5 –¢–û–ö–ï–ù–û–í –ü–û –£–°–ü–ï–®–ù–û–°–¢–ò:",
        ]

        for i, token in enumerate(analysis['best_tokens'][:5], 1):
            report_lines.append(
                f"{i}. {token['symbol']}: {token['success_rate']:.1f}% —É—Å–ø–µ—Ö–∞ "
                f"({token['successful_signals']}/{token['total_signals']} —Å–∏–≥–Ω–∞–ª–æ–≤)"
            )

        report_lines.extend([
            "",
            "–õ–£–ß–®–ï–ï –í–†–ï–ú–Ø –î–õ–Ø –¢–û–†–ì–û–í–õ–ò (UTC):",
            f"–õ—É—á—à–∏–µ —á–∞—Å—ã: {', '.join(map(str, analysis['best_time_slots']['best_hours']))}",
            f"–•—É–¥—à–∏–µ —á–∞—Å—ã: {', '.join(map(str, analysis['best_time_slots']['worst_hours']))}",
            "",
            "–ö–û–†–†–ï–õ–Ø–¶–ò–ò –° –£–°–ü–ï–®–ù–û–°–¢–¨–Æ:",
        ])

        for feature, corr in list(analysis['correlation_analysis'].items())[:5]:
            report_lines.append(f"- {feature}: {corr:.3f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        with open(f'{output_path}/analysis_report_{timestamp}.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

        print(f"\nüìÑ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ '{output_path}/'")
        print('\n'.join(report_lines[:20]) + '\n...')

        return f'{output_path}/analysis_report_{timestamp}'

    def export_for_ml(self, df: pd.DataFrame, output_path: str = 'ml_data'):
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        os.makedirs(output_path, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        df.to_csv(f'{output_path}/signals_with_outcomes.csv', index=False)

        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Ç–æ–ª—å–∫–æ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è ML
        feature_columns = [
            'OI_contracts_binance_change', 'OI_contracts_bybit_change',
            'funding_rate_binance_now', 'funding_rate_bybit_now',
            'price_change_10min', 'signal_volume_usd',
            'oi_usdt_change_current_to_average', 'oi_usdt_change_current_to_yesterday',
            'spot_volume_usdt_change_current_to_average',
            'spot_price_usdt_change_1h', 'spot_price_usdt_change_24h',
            'spot_price_usdt_change_7d'
        ]

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        existing_features = [col for col in feature_columns if col in df.columns]

        ml_dataset = df[existing_features + ['is_successful']].dropna()
        ml_dataset.to_csv(f'{output_path}/ml_features.csv', index=False)

        print(f"\nüíæ –î–∞–Ω–Ω—ã–µ –¥–ª—è ML —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ '{output_path}/'")
        print(f"   - –ü–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"   - ML –¥–∞—Ç–∞—Å–µ—Ç: {len(ml_dataset)} –∑–∞–ø–∏—Å–µ–π")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤...")

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        analyzer = CryptoSignalsAnalyzer()

        # –ü–æ–ª—É—á–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
        signals = analyzer.get_recent_signals(days=30)

        if len(signals) == 0:
            print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        signals_with_outcomes = analyzer.calculate_signal_outcomes(signals)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        analysis = analyzer.analyze_patterns(signals_with_outcomes)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report_path = analyzer.generate_report(analysis)

        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è ML
        analyzer.export_for_ml(signals_with_outcomes)

        print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(signals)} —Å–∏–≥–Ω–∞–ª–æ–≤")
        print(f"üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {analysis['general_stats']['success_rate']:.1f}%")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        signals_with_outcomes.to_pickle('latest_analysis.pkl')
        print("\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'latest_analysis.pkl'")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()