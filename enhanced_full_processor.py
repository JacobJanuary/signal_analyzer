#!/usr/bin/env python3
"""
Enhanced Full Signals Processor - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü
"""

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
import json
import os
from typing import Dict, List, Tuple, Optional, Any
from dotenv import load_dotenv
from urllib.parse import quote_plus
from tqdm import tqdm

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
import xgboost as xgb
import lightgbm as lgb
from xgboost.callback import EarlyStopping
from sklearn.neural_network import MLPClassifier
import joblib

load_dotenv()

AVOID_ZERO_DIV_SMALL = 1e-5
AVOID_ZERO_DIV_TINY = 1e-9


class EnhancedFullSignalsProcessor:
    """
    –ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å–∏–≥–Ω–∞–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """

    def __init__(self):
        self.engine = self._create_db_connection()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞
        self.profit_targets = [3, 5, 10]
        self.stop_losses = [2, 3, 5]
        self.time_windows_hours = [6, 12, 24, 48]

        # ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.models: Dict[str, Any] = {}
        self.scaler = StandardScaler()
        self.label_encoders = {}

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.define_all_features()

    def _create_db_connection(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            user = os.getenv('DB_USER')
            password = quote_plus(os.getenv('DB_PASSWORD', ''))
            host = os.getenv('DB_HOST')
            port = os.getenv('DB_PORT', 3306)
            database = os.getenv('DB_NAME')

            if not all([user, password, host, database]):
                raise ValueError("–ù–µ –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î –∑–∞–¥–∞–Ω—ã")

            connection_string = f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"
            engine = create_engine(connection_string, pool_pre_ping=True)

            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return engine
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
            raise

    def define_all_features(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –ë–î"""

        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ signals_10min
        self.base_signal_features = [
            'OI_contracts_binance_now', 'OI_contracts_binance_prev', 'OI_contracts_binance_change',
            'OI_contracts_bybit_now', 'OI_contracts_bybit_prev', 'OI_contracts_bybit_change',
            'funding_rate_binance_now', 'funding_rate_binance_prev', 'funding_rate_binance_change',
            'funding_rate_bybit_now', 'funding_rate_bybit_prev', 'funding_rate_bybit_change',
            'signal_volume_usd', 'signal_price', 'signal_price_prev', 'price_change_10min',
            'market_cap_usd'
        ]

        # –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ signals_10min_enriched
        self.enriched_features = [
            # OI –º–µ—Ç—Ä–∏–∫–∏
            'oi_usdt_average', 'oi_usdt_current', 'oi_usdt_yesterday',
            'oi_usdt_change_current_to_yesterday', 'oi_usdt_change_current_to_average',

            # –°–ø–æ—Ç–æ–≤—ã–µ –æ–±—ä–µ–º—ã –≤ USDT
            'spot_volume_usdt_average', 'spot_volume_usdt_current', 'spot_volume_usdt_yesterday',
            'spot_volume_usdt_change_current_to_yesterday', 'spot_volume_usdt_change_current_to_average',

            # –°–ø–æ—Ç–æ–≤—ã–µ –æ–±—ä–µ–º—ã –≤ BTC - –ù–û–í–û–ï!
            'spot_volume_btc_average', 'spot_volume_btc_current', 'spot_volume_btc_yesterday',
            'spot_volume_btc_change_current_to_yesterday', 'spot_volume_btc_change_current_to_average',

            # –°–ø–æ—Ç–æ–≤—ã–µ —Ü–µ–Ω—ã
            'spot_price_usdt_average', 'spot_price_usdt_current', 'spot_price_usdt_yesterday',
            'spot_price_usdt_change_1h', 'spot_price_usdt_change_24h',
            'spot_price_usdt_change_7d', 'spot_price_usdt_change_30d',

            # CMC –¥–∞–Ω–Ω—ã–µ
            'cmc_price_min_1h', 'cmc_price_max_1h', 'cmc_price_min_24h', 'cmc_price_max_24h',
            'cmc_price_min_7d', 'cmc_price_max_7d', 'cmc_price_min_30d', 'cmc_price_max_30d',
            'cmc_percent_change_1d', 'cmc_percent_change_7d', 'cmc_percent_change_30d'
        ]

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ - –ù–û–í–û–ï!
        self.source_features = [
            'oi_source_usdt', 'spot_volume_source_usdt',
            'spot_volume_source_btc', 'spot_price_source_usdt'
        ]

        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤—ã—á–∏—Å–ª—è–µ–º—ã–µ)
        self.technical_features = [
            'hour', 'day_of_week', 'is_weekend',
            'oi_binance_bybit_ratio', 'funding_binance_bybit_ratio',
            'price_momentum', 'volatility_1h', 'volatility_24h', 'signal_strength',
            # –ù–æ–≤—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            'volume_btc_usdt_ratio', 'oi_to_volume_ratio', 'funding_divergence',
            'price_range_1h', 'price_range_24h', 'volume_profile_score',
            'multi_source_confidence', 'exchange_dominance'
        ]

    def get_all_signals_complete(self, limit: Optional[int] = None) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Å–∏–≥–Ω–∞–ª–∞—Ö
        """
        max_hours_ago = max(self.time_windows_hours)
        print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –ü–û–õ–ù–´–• –¥–∞–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ –ë–î (—Å—Ç–∞—Ä—à–µ {max_hours_ago} —á–∞—Å–æ–≤)... (–õ–∏–º–∏—Ç: {limit or '–ù–µ—Ç'})")

        limit_clause = "LIMIT :limit_val" if limit else ""

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å–æ –í–°–ï–ú–ò –ø–æ–ª—è–º–∏
        query = text(f"""
        SELECT 
            -- –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª–∞
            s.id as signal_id, 
            s.token_id, 
            s.symbol, 
            s.timestamp as signal_timestamp,

            -- OI –¥–∞–Ω–Ω—ã–µ –ø–æ –±–∏—Ä–∂–∞–º
            s.OI_contracts_binance_now, 
            s.OI_contracts_binance_prev, 
            s.OI_contracts_binance_change,
            s.OI_contracts_bybit_now, 
            s.OI_contracts_bybit_prev, 
            s.OI_contracts_bybit_change,

            -- Funding rates
            s.funding_rate_binance_now, 
            s.funding_rate_binance_prev, 
            s.funding_rate_binance_change,
            s.funding_rate_bybit_now, 
            s.funding_rate_bybit_prev, 
            s.funding_rate_bybit_change,

            -- –û–±—ä–µ–º—ã –∏ —Ü–µ–Ω—ã
            s.volume_usd as signal_volume_usd, 
            s.price_usd_now as signal_price,
            s.price_usd_prev as signal_price_prev, 
            s.price_usd_change as price_change_10min,
            s.market_cap_usd,

            -- –í–°–ï –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            e.oi_usdt_average, 
            e.oi_usdt_current, 
            e.oi_usdt_yesterday,
            e.oi_usdt_change_current_to_yesterday, 
            e.oi_usdt_change_current_to_average,
            e.oi_source_usdt,  -- –ù–û–í–û–ï: –∏—Å—Ç–æ—á–Ω–∏–∫ OI

            -- –°–ø–æ—Ç–æ–≤—ã–µ –æ–±—ä–µ–º—ã USDT
            e.spot_volume_usdt_average, 
            e.spot_volume_usdt_current, 
            e.spot_volume_usdt_yesterday,
            e.spot_volume_usdt_change_current_to_yesterday, 
            e.spot_volume_usdt_change_current_to_average,
            e.spot_volume_source_usdt,  -- –ù–û–í–û–ï: –∏—Å—Ç–æ—á–Ω–∏–∫ –æ–±—ä–µ–º–∞ USDT

            -- –°–ø–æ—Ç–æ–≤—ã–µ –æ–±—ä–µ–º—ã BTC - –ù–û–í–û–ï!
            e.spot_volume_btc_average, 
            e.spot_volume_btc_current, 
            e.spot_volume_btc_yesterday,
            e.spot_volume_btc_change_current_to_yesterday, 
            e.spot_volume_btc_change_current_to_average,
            e.spot_volume_source_btc,  -- –ù–û–í–û–ï: –∏—Å—Ç–æ—á–Ω–∏–∫ –æ–±—ä–µ–º–∞ BTC

            -- –°–ø–æ—Ç–æ–≤—ã–µ —Ü–µ–Ω—ã
            e.spot_price_usdt_average, 
            e.spot_price_usdt_current, 
            e.spot_price_usdt_yesterday,
            e.spot_price_usdt_change_1h, 
            e.spot_price_usdt_change_24h, 
            e.spot_price_usdt_change_7d, 
            e.spot_price_usdt_change_30d,
            e.spot_price_source_usdt,  -- –ù–û–í–û–ï: –∏—Å—Ç–æ—á–Ω–∏–∫ —Ü–µ–Ω—ã

            -- CMC –º–µ—Ç—Ä–∏–∫–∏
            e.cmc_price_min_1h, 
            e.cmc_price_max_1h, 
            e.cmc_price_min_24h, 
            e.cmc_price_max_24h,
            e.cmc_price_min_7d, 
            e.cmc_price_max_7d, 
            e.cmc_price_min_30d, 
            e.cmc_price_max_30d,
            e.cmc_percent_change_1d, 
            e.cmc_percent_change_7d, 
            e.cmc_percent_change_30d

        FROM signals_10min s
        LEFT JOIN signals_10min_enriched e ON s.id = e.signal_id
        WHERE s.OI_contracts_binance_change > 0
        ORDER BY s.timestamp DESC
        {limit_clause}
        """)

        params = {"limit_val": limit} if limit else {}
        df = pd.read_sql(query, self.engine, params=params)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–∏–≥–Ω–∞–ª–æ–≤ —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏")

        if not df.empty:
            df = self._add_enhanced_technical_features(df)
            df = self._encode_categorical_features(df)

        return df

    def _add_enhanced_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("üîß –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –ë–∞–∑–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df['signal_timestamp'] = pd.to_datetime(df['signal_timestamp'])
        df['hour'] = df['signal_timestamp'].dt.hour
        df['day_of_week'] = df['signal_timestamp'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

        # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
        df['oi_binance_bybit_ratio'] = df['OI_contracts_binance_change'] / (
                df['OI_contracts_bybit_change'] + AVOID_ZERO_DIV_SMALL)
        df['funding_binance_bybit_ratio'] = df['funding_rate_binance_now'] / (
                df['funding_rate_bybit_now'] + AVOID_ZERO_DIV_TINY)

        # –ú–æ–º–µ–Ω—Ç—É–º –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        df['price_momentum'] = df['spot_price_usdt_change_1h'] * df['spot_price_usdt_change_24h']
        df['volatility_1h'] = ((df['cmc_price_max_1h'] - df['cmc_price_min_1h']) / df['signal_price'] * 100).fillna(0)
        df['volatility_24h'] = ((df['cmc_price_max_24h'] - df['cmc_price_min_24h']) / df['signal_price'] * 100).fillna(
            0)

        # –ù–û–í–´–ï –ø—Ä–∏–∑–Ω–∞–∫–∏

        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–≤ BTC/USDT
        df['volume_btc_usdt_ratio'] = (df['spot_volume_btc_current'] * df['signal_price']) / (
                df['spot_volume_usdt_current'] + AVOID_ZERO_DIV_SMALL)

        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ OI –∫ –æ–±—ä–µ–º—É
        df['oi_to_volume_ratio'] = df['oi_usdt_current'] / (
                df['spot_volume_usdt_current'] + AVOID_ZERO_DIV_SMALL)

        # –î–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è funding rates –º–µ–∂–¥—É –±–∏—Ä–∂–∞–º–∏
        df['funding_divergence'] = abs(
            df['funding_rate_binance_now'] - df['funding_rate_bybit_now']
        )

        # –¶–µ–Ω–æ–≤—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        df['price_range_1h'] = (df['cmc_price_max_1h'] - df['cmc_price_min_1h']).fillna(0)
        df['price_range_24h'] = (df['cmc_price_max_24h'] - df['cmc_price_min_24h']).fillna(0)

        # –ü—Ä–æ—Ñ–∏–ª—å –æ–±—ä–µ–º–∞ (–Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π –æ–±—ä–µ–º –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ)
        df['volume_profile_score'] = (
                abs(df['spot_volume_usdt_change_current_to_average']) * 0.5 +
                abs(df['spot_volume_btc_change_current_to_average']) * 0.5
        ).fillna(0)

        # –î–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∏—Ä–∂–∏ (–∫–∞–∫–∞—è –±–∏—Ä–∂–∞ –∏–º–µ–µ—Ç –±–æ–ª—å—à–∏–π OI change)
        df['exchange_dominance'] = np.where(
            df['OI_contracts_binance_change'] > df['OI_contracts_bybit_change'],
            1,  # Binance –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç
            -1  # Bybit –¥–æ–º–∏–Ω–∏—Ä—É–µ—Ç
        )

        # –°–∏–ª–∞ —Å–∏–≥–Ω–∞–ª–∞ —Å —É—á–µ—Ç–æ–º –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df['signal_strength'] = (
                df['OI_contracts_binance_change'] * 0.3 +
                df['OI_contracts_bybit_change'] * 0.3 +
                df['spot_volume_usdt_change_current_to_average'] * 0.2 +
                df['price_change_10min'] * 0.1 +
                df['volume_profile_score'] * 0.1
        ).fillna(0)

        return df

    def _encode_categorical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ö–æ–¥–∏—Ä—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        print("üè∑Ô∏è –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        source_mapping = {
            'binance': 2,
            'bybit': 1,
            'coinmarketcap': 3,
            None: 0
        }

        # –ö–æ–¥–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for source_col in self.source_features:
            if source_col in df.columns:
                df[f'{source_col}_encoded'] = df[source_col].map(source_mapping).fillna(0)

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        # (CMC —Å—á–∏—Ç–∞–µ—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–º –¥–ª—è —Ü–µ–Ω, –±–∏—Ä–∂–∏ - –¥–ª—è –æ–±—ä–µ–º–æ–≤)
        df['multi_source_confidence'] = 0

        if 'spot_price_source_usdt' in df.columns:
            df['multi_source_confidence'] += (df['spot_price_source_usdt'] == 'coinmarketcap').astype(int) * 0.5

        if 'spot_volume_source_usdt' in df.columns:
            df['multi_source_confidence'] += (df['spot_volume_source_usdt'].isin(['binance', 'bybit'])).astype(
                int) * 0.5

        return df

    def get_enhanced_futures_data(self, token_id: int, signal_time: datetime,
                                  hours_after: int = 48) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ—å—é—á–µ—Ä—Å–æ–≤ –≤–∫–ª—é—á–∞—è –≤—Å–µ –ø–æ–ª—è
        """
        query = text("""
                     SELECT fd.timestamp,
                            fd.price_usd,
                            fd.volume_usd,
                            fd.volume_btc,              -- –ù–û–í–û–ï
                            fd.open_interest_usd,
                            fd.open_interest_contracts, -- –ù–û–í–û–ï
                            fd.funding_rate,
                            fd.market_cap_usd,
                            fd.btc_price                -- –ù–û–í–û–ï: —Ü–µ–Ω–∞ BTC –Ω–∞ –º–æ–º–µ–Ω—Ç –∑–∞–ø–∏—Å–∏
                     FROM futures_data fd
                              JOIN futures_pairs fp ON fd.pair_id = fp.id
                     WHERE fp.token_id = :token_id
                       AND fd.timestamp >= :start_time
                       AND fd.timestamp <= :end_time
                     ORDER BY fd.timestamp
                     """)

        params = {
            "token_id": token_id,
            "start_time": signal_time,
            "end_time": signal_time + timedelta(hours=hours_after)
        }

        return pd.read_sql(query, self.engine, params=params)

    def _calculate_enhanced_outcomes(self, signal: pd.Series,
                                     futures_data: pd.DataFrame) -> Dict[str, Any]:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        entry_price = signal['signal_price']
        outcomes: Dict[str, Any] = {}

        futures_data['timestamp'] = pd.to_datetime(futures_data['timestamp'])
        signal_time = pd.to_datetime(signal['signal_timestamp'])

        futures_data['time_diff_min'] = (futures_data['timestamp'] - signal_time).dt.total_seconds() / 60
        futures_data['pnl_pct'] = ((futures_data['price_usd'] - entry_price) / entry_price) * 100

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        outcomes['max_profit_pct'] = futures_data['pnl_pct'].max()
        outcomes['max_drawdown_pct'] = futures_data['pnl_pct'].min()
        outcomes['final_pnl_pct'] = futures_data['pnl_pct'].iloc[-1] if not futures_data.empty else 0

        # –í—Ä–µ–º—è –¥–æ —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
        if pd.notna(outcomes['max_profit_pct']) and outcomes['max_profit_pct'] > 0:
            outcomes['time_to_max_profit_min'] = futures_data.loc[futures_data['pnl_pct'].idxmax(), 'time_diff_min']
        else:
            outcomes['time_to_max_profit_min'] = None

        if pd.notna(outcomes['max_drawdown_pct']) and outcomes['max_drawdown_pct'] < 0:
            outcomes['time_to_max_drawdown_min'] = futures_data.loc[futures_data['pnl_pct'].idxmin(), 'time_diff_min']
        else:
            outcomes['time_to_max_drawdown_min'] = None

        # –†–∞—Å—á–µ—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –ø–æ —Ä–∞–∑–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        for profit_target in self.profit_targets:
            for stop_loss in self.stop_losses:
                for time_window in self.time_windows_hours:
                    success_key = f'success_{profit_target}p_{stop_loss}sl_{time_window}h'
                    window_data = futures_data[futures_data['time_diff_min'] <= time_window * 60]

                    if window_data.empty:
                        outcomes[success_key] = 0
                        continue

                    profit_times = window_data.index[window_data['pnl_pct'] >= profit_target]
                    stop_times = window_data.index[window_data['pnl_pct'] <= -stop_loss]

                    first_profit_time = profit_times.min() if not profit_times.empty else np.inf
                    first_stop_time = stop_times.min() if not stop_times.empty else np.inf

                    is_successful = first_profit_time < first_stop_time
                    outcomes[success_key] = int(is_successful)

                    if profit_target == 5 and stop_loss == 3 and time_window == 24:
                        outcomes['is_successful_main'] = int(is_successful)
                        outcomes['hit_profit_target'] = int(not profit_times.empty)
                        outcomes['hit_stop_loss'] = int(not stop_times.empty)

        # –ù–û–í–´–ï —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if len(futures_data) > 1:
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            outcomes['volatility_after'] = futures_data['pnl_pct'].std()

            # –¢—Ä–µ–Ω–¥—ã –æ–±—ä–µ–º–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ None
            last_vol_usd = futures_data['volume_usd'].iloc[-1]
            first_vol_usd = futures_data['volume_usd'].iloc[0]
            if pd.notna(last_vol_usd) and pd.notna(first_vol_usd):
                outcomes['volume_usd_trend'] = 1 if last_vol_usd > first_vol_usd else -1
            else:
                outcomes['volume_usd_trend'] = 0

            if 'volume_btc' in futures_data.columns:
                last_vol_btc = futures_data['volume_btc'].iloc[-1]
                first_vol_btc = futures_data['volume_btc'].iloc[0]
                if pd.notna(last_vol_btc) and pd.notna(first_vol_btc):
                    outcomes['volume_btc_trend'] = 1 if last_vol_btc > first_vol_btc else -1
                else:
                    outcomes['volume_btc_trend'] = 0
                outcomes['avg_volume_btc_after'] = futures_data['volume_btc'].mean()

            # OI –∏–∑–º–µ–Ω–µ–Ω–∏—è
            oi_initial = futures_data['open_interest_usd'].iloc[0]
            if oi_initial > 0:
                outcomes['oi_change_after'] = ((futures_data['open_interest_usd'].iloc[
                                                    -1] - oi_initial) / oi_initial) * 100
            else:
                outcomes['oi_change_after'] = 0

            # OI –≤ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ö
            if 'open_interest_contracts' in futures_data.columns:
                oi_contracts_initial = futures_data['open_interest_contracts'].iloc[0]
                if oi_contracts_initial > 0:
                    outcomes['oi_contracts_change_after'] = (
                            (futures_data['open_interest_contracts'].iloc[-1] - oi_contracts_initial) /
                            oi_contracts_initial * 100
                    )
                else:
                    outcomes['oi_contracts_change_after'] = 0

            # Funding rate –¥–∏–Ω–∞–º–∏–∫–∞
            if 'funding_rate' in futures_data.columns:
                outcomes['funding_rate_change_after'] = (
                        futures_data['funding_rate'].iloc[-1] - futures_data['funding_rate'].iloc[0]
                )
                outcomes['avg_funding_rate_after'] = futures_data['funding_rate'].mean()

            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å BTC
            if 'btc_price' in futures_data.columns:
                last_btc_price = futures_data['btc_price'].iloc[-1]
                first_btc_price = futures_data['btc_price'].iloc[0]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å –Ω–µ —Ä–∞–≤–µ–Ω –Ω—É–ª—é
                if pd.notna(first_btc_price) and first_btc_price > 0 and pd.notna(last_btc_price):
                    btc_change = ((last_btc_price - first_btc_price) / first_btc_price) * 100
                    outcomes['performance_vs_btc'] = outcomes.get('final_pnl_pct', 0) - btc_change
                else:
                    outcomes['performance_vs_btc'] = outcomes.get('final_pnl_pct', 0)

                    # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –î–û–ë–ê–í–õ–ï–ù–û –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ---
                    # –õ–æ–≥–∏—Ä—É–µ–º, –µ—Å–ª–∏ —Ü–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞ (0 –∏–ª–∏ NULL)
                    if pd.notna(first_btc_price) and first_btc_price == 0:
                        print(
                            f"\n‚ö†Ô∏è  [Warning] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ù–£–õ–ï–í–ê–Ø —Ü–µ–Ω–∞ BTC –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞ ID: {signal.get('signal_id')}, —Å–∏–º–≤–æ–ª: {signal.get('symbol')}.")
                    elif pd.isna(first_btc_price):
                        print(
                            f"\n‚ö†Ô∏è  [Warning] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ NULL —Ü–µ–Ω–∞ BTC –¥–ª—è —Å–∏–≥–Ω–∞–ª–∞ ID: {signal.get('signal_id')}, —Å–∏–º–≤–æ–ª: {signal.get('symbol')}.")
                    # --- –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø ---

        return outcomes

    def calculate_signal_outcomes_enhanced(self, signals_df: pd.DataFrame,
                                           batch_size: int = 100) -> pd.DataFrame:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤—Å–µ—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
        """
        print(f"\nüîÑ –†–∞—Å—á–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è {len(signals_df)} —Å–∏–≥–Ω–∞–ª–æ–≤...")
        all_results = []

        max_hours = max(self.time_windows_hours)

        for _, signal in tqdm(signals_df.iterrows(), total=len(signals_df)):
            try:
                futures_data = self.get_enhanced_futures_data(
                    signal['token_id'],
                    signal['signal_timestamp'],
                    hours_after=max_hours
                )

                if not futures_data.empty:
                    outcomes = self._calculate_enhanced_outcomes(signal, futures_data)
                    signal_dict = signal.to_dict()
                    signal_dict.update(outcomes)
                    all_results.append(signal_dict)

            except Exception as e:
                print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–∏–≥–Ω–∞–ª–∞ {signal['signal_id']}: {e}")
                continue

        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(all_results)} —Å–∏–≥–Ω–∞–ª–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
        return pd.DataFrame(all_results)

    def prepare_full_feature_set(self, df: pd.DataFrame,
                                 target_column: str = 'is_successful_main') -> Tuple[
        pd.DataFrame, pd.Series, List[str]]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML
        """
        print("\nü§ñ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML...")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        all_features = (
                self.base_signal_features +
                self.enriched_features +
                self.technical_features +
                [f'{col}_encoded' for col in self.source_features]  # –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ outcome –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        outcome_features = [
            'volume_btc_trend', 'avg_volume_btc_after', 'oi_contracts_change_after',
            'funding_rate_change_after', 'avg_funding_rate_after', 'performance_vs_btc'
        ]

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        available_features = [col for col in all_features if col in df.columns]

        # –î–æ–±–∞–≤–ª—è–µ–º outcome –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑ —É–∂–µ –±—ã–ª –ø—Ä–æ–≤–µ–¥–µ–Ω
        for feat in outcome_features:
            if feat in df.columns:
                available_features.append(feat)

        df_clean = df.dropna(subset=[target_column])

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X = df_clean[available_features].fillna(df_clean[available_features].median())
        y = df_clean[target_column]

        print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ —Å {len(available_features)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        print(f"   –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len([f for f in self.base_signal_features if f in available_features])}")
        print(f"   –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len([f for f in self.enriched_features if f in available_features])}")
        print(f"   –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len([f for f in self.technical_features if f in available_features])}")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len([f for f in available_features if f.endswith('_encoded')])}")

        if not y.empty:
            class_distribution = y.value_counts(normalize=True)
            print(
                f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: 0: {class_distribution.get(0, 0):.2%}, 1: {class_distribution.get(1, 0):.2%}")

        return X, y, available_features

    def train_enhanced_models(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("\nüéØ –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö...")

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        results = {}

        # 1. Random Forest —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        print("\n1Ô∏è‚É£  Random Forest...")
        rf_model = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=20,
            min_samples_leaf=5,
            max_features='sqrt',
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train, y_train)
        results['random_forest'] = self._evaluate_model(rf_model, X_test, y_test, X.columns)

        # 2. XGBoost —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ–¥ –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("2Ô∏è‚É£  XGBoost...")
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum() if (y_train == 1).sum() > 0 else 1

        xgb_model = xgb.XGBClassifier(
            n_estimators=300,
            max_depth=8,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.6,
            scale_pos_weight=scale_pos_weight,
            random_state=42,
            eval_metric='logloss',
            reg_alpha=0.1,
            reg_lambda=1.0
        )

        eval_set = [(X_test, y_test)]

        # --- –≠–¢–û –í–ï–†–ù–´–ô –°–ò–ù–¢–ê–ö–°–ò–° –î–õ–Ø –í–ê–®–ï–ô –í–ï–†–°–ò–ò XGBOOST 3.0.2 ---
        xgb_model.fit(
            X_train, y_train,
            eval_set=eval_set,
            callbacks=[EarlyStopping(rounds=50, save_best=True)],
            verbose=False
        )
        # ---

        results['xgboost'] = self._evaluate_model(xgb_model, X_test, y_test)

        # 3. LightGBM - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("3Ô∏è‚É£  LightGBM...")
        lgb_model = lgb.LGBMClassifier(
            n_estimators=300,
            max_depth=8,
            learning_rate=0.05,
            num_leaves=31,
            feature_fraction=0.7,
            bagging_fraction=0.8,
            bagging_freq=5,
            class_weight='balanced',
            random_state=42,
            verbosity=-1
        )
        lgb_model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        results['lightgbm'] = self._evaluate_model(lgb_model, X_test, y_test)

        # 4. Neural Network —Å –±–æ–ª—å—à–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
        print("4Ô∏è‚É£  Neural Network...")
        nn_model = MLPClassifier(
            hidden_layer_sizes=(256, 128, 64, 32),
            activation='relu',
            solver='adam',
            alpha=0.001,
            batch_size='auto',
            learning_rate='adaptive',
            learning_rate_init=0.001,
            max_iter=1000,
            early_stopping=True,
            validation_fraction=0.1,
            n_iter_no_change=20,
            random_state=42
        )
        nn_model.fit(X_train_scaled, y_train)
        results['neural_network'] = self._evaluate_model(nn_model, X_test_scaled, y_test)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        self.models = {name: res['model'] for name, res in results.items()}

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._print_enhanced_results(results)

        return results

    def _evaluate_model(self, model: Any, X_test: pd.DataFrame, y_test: pd.Series,
                        feature_names: Optional[pd.Index] = None) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        pred = model.predict(X_test)
        pred_proba = model.predict_proba(X_test)[:, 1]

        report = classification_report(y_test, pred, output_dict=True, zero_division=0)

        metrics = {
            'model': model,
            'accuracy': report['accuracy'],
            'auc': roc_auc_score(y_test, pred_proba),
            'precision_0': report['0']['precision'],
            'recall_0': report['0']['recall'],
            'precision_1': report['1']['precision'],
            'recall_1': report['1']['recall'],
            'f1_score': report['1']['f1-score'],
            'report': report,
        }

        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥—Ä–µ–≤–µ—Å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        if feature_names is not None and hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)

            metrics['feature_importance'] = importance_df
            metrics['top_features'] = importance_df.head(20)

        return metrics

    def _print_enhanced_results(self, results: Dict):
        """–í—ã–≤–æ–¥–∏—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è"""
        print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö:")
        print("=" * 80)

        for model_name, metrics in results.items():
            print(f"\n{model_name.upper()}:")
            print(f"  Accuracy: {metrics['accuracy']:.4f}")
            print(f"  AUC: {metrics['auc']:.4f}")
            print(f"  Class 0 - Precision: {metrics['precision_0']:.3f}, Recall: {metrics['recall_0']:.3f}")
            print(f"  Class 1 - Precision: {metrics['precision_1']:.3f}, Recall: {metrics['recall_1']:.3f}")
            print(f"  F1-Score (Class 1): {metrics['f1_score']:.3f}")

            # –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å feature importance
            if 'top_features' in metrics:
                print(f"\n  –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                for idx, row in metrics['top_features'].head(10).iterrows():
                    print(f"    {idx + 1}. {row['feature']}: {row['importance']:.4f}")

    def analyze_feature_groups_importance(self, results: Dict) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("\nüìä –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≥—Ä—É–ø–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        if 'random_forest' not in results or 'feature_importance' not in results['random_forest']:
            return pd.DataFrame()

        importance_df = results['random_forest']['feature_importance'].copy()

        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º
        def classify_feature(feature_name):
            if feature_name in self.base_signal_features:
                return '–ë–∞–∑–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã'
            elif feature_name in self.enriched_features:
                return '–û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'
            elif feature_name in self.technical_features:
                return '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã'
            elif feature_name.endswith('_encoded'):
                return '–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö'
            elif 'btc' in feature_name.lower():
                return 'BTC –∫–æ–Ω—Ç–µ–∫—Å—Ç'
            elif 'after' in feature_name:
                return 'Outcome –º–µ—Ç—Ä–∏–∫–∏'
            else:
                return '–ü—Ä–æ—á–∏–µ'

        importance_df['feature_group'] = importance_df['feature'].apply(classify_feature)

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –≥—Ä—É–ø–ø–∞–º
        group_importance = importance_df.groupby('feature_group').agg({
            'importance': ['sum', 'mean', 'count']
        }).round(4)

        group_importance.columns = ['total_importance', 'avg_importance', 'feature_count']
        group_importance = group_importance.sort_values('total_importance', ascending=False)

        print("\n–í–∞–∂–Ω–æ—Å—Ç—å –≥—Ä—É–ø–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        print(group_importance)

        return group_importance

    def generate_comprehensive_report(self, df: pd.DataFrame, results: Dict,
                                      group_importance: pd.DataFrame) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")

        report_lines = [
            "=" * 100,
            "–ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –ö–†–ò–ü–¢–û–í–ê–õ–Æ–¢–ù–´–• –°–ò–ì–ù–ê–õ–û–í",
            "–° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú –ü–û–õ–ù–û–ì–û –ù–ê–ë–û–†–ê –î–ê–ù–ù–´–•",
            f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 100, "",

            "–û–ë–†–ê–ë–û–¢–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï:",
            f"- –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(df)}",
            f"- –ü–µ—Ä–∏–æ–¥: {df['signal_timestamp'].min()} - {df['signal_timestamp'].max()}",
            f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {df['symbol'].nunique()}",
            f"- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len([col for col in df.columns if col not in ['signal_id', 'symbol', 'signal_timestamp']])}",
            "",

            "–ò–°–¢–û–ß–ù–ò–ö–ò –î–ê–ù–ù–´–•:",
        ]

        # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        for source_col in self.source_features:
            if source_col in df.columns:
                source_dist = df[source_col].value_counts()
                report_lines.append(f"\n{source_col}:")
                for source, count in source_dist.items():
                    report_lines.append(f"  - {source}: {count} ({count / len(df) * 100:.1f}%)")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if not group_importance.empty:
            report_lines.extend([
                "",
                "–í–ê–ñ–ù–û–°–¢–¨ –ì–†–£–ü–ü –ü–†–ò–ó–ù–ê–ö–û–í (Random Forest):",
            ])
            for group, row in group_importance.iterrows():
                report_lines.append(
                    f"- {group}: {row['total_importance']:.3f} "
                    f"(—Å—Ä–µ–¥–Ω–µ–µ: {row['avg_importance']:.4f}, –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {int(row['feature_count'])})"
                )

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–∞–∑–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        report_lines.extend([
            "",
            "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û –ö–†–ò–¢–ï–†–ò–Ø–ú –£–°–ü–ï–•–ê:",
        ])

        for profit in self.profit_targets:
            for stop in self.stop_losses:
                for window in self.time_windows_hours:
                    col_name = f'success_{profit}p_{stop}sl_{window}h'
                    if col_name in df.columns:
                        success_rate = df[col_name].mean() * 100
                        count = df[col_name].sum()
                        report_lines.append(
                            f"- {profit}% –ø—Ä–∏–±—ã–ª—å / {stop}% —Å—Ç–æ–ø / {window}—á: "
                            f"{success_rate:.1f}% —É—Å–ø–µ—Ö–∞ ({count} –∏–∑ {len(df)})"
                        )

        # ML —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        report_lines.extend([
            "",
            "–†–ï–ó–£–õ–¨–¢–ê–¢–´ ML –ú–û–î–ï–õ–ï–ô:",
        ])

        best_model = None
        best_auc = 0

        for model_name, metrics in results.items():
            report_lines.extend([
                f"\n{model_name.upper()}:",
                f"  - Accuracy: {metrics['accuracy']:.4f}",
                f"  - AUC: {metrics['auc']:.4f}",
                f"  - Precision (Class 1): {metrics['precision_1']:.3f}",
                f"  - Recall (Class 1): {metrics['recall_1']:.3f}",
                f"  - F1-Score: {metrics['f1_score']:.3f}"
            ])

            if metrics['auc'] > best_auc:
                best_auc = metrics['auc']
                best_model = model_name

        # –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'random_forest' in results and 'top_features' in results['random_forest']:
            report_lines.extend([
                "",
                "–¢–û–ü-20 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:",
            ])
            for idx, row in results['random_forest']['top_features'].head(20).iterrows():
                report_lines.append(f"  {idx + 1}. {row['feature']}: {row['importance']:.4f}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        report_lines.extend([
            "",
            "–¢–û–ü-10 –¢–û–ö–ï–ù–û–í –ü–û –ö–û–õ–ò–ß–ï–°–¢–í–£ –°–ò–ì–ù–ê–õ–û–í:",
        ])
        top_tokens = df['symbol'].value_counts().head(10)
        for i, (token, count) in enumerate(top_tokens.items(), 1):
            token_df = df[df['symbol'] == token]
            if 'is_successful_main' in token_df.columns:
                success_rate = token_df['is_successful_main'].mean() * 100
                avg_oi_change = (token_df['OI_contracts_binance_change'] +
                                 token_df['OI_contracts_bybit_change']).mean()
                report_lines.append(
                    f"{i}. {token}: {count} —Å–∏–≥–Ω–∞–ª–æ–≤ "
                    f"({success_rate:.1f}% —É—Å–ø–µ—Ö–∞, —Å—Ä–µ–¥–Ω–∏–π OI change: {avg_oi_change:.2f})"
                )

        # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report_lines.extend([
            "",
            "=" * 100,
            "–ò–¢–û–ì–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:",
            f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model.upper()} (AUC: {best_auc:.4f})",
            "‚úÖ –ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã",
            "‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏",
            "=" * 100
        ])

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        os.makedirs('reports', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = f'reports/comprehensive_analysis_report_{timestamp}.txt'

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

        print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        return report_path


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –í–°–ï–• –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    print("   –≤–∫–ª—é—á–∞—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É NULL –∑–Ω–∞—á–µ–Ω–∏–π")
    print("=" * 100)

    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º null-aware —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        from null_aware_processor import enhance_processor_with_null_awareness

        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π null-aware
        NullAwareProcessor = enhance_processor_with_null_awareness(EnhancedFullSignalsProcessor)
        processor = NullAwareProcessor()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        limit_signals = None  # None –¥–ª—è –≤—Å–µ—Ö —Å–∏–≥–Ω–∞–ª–æ–≤

        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ
        print("\n" + "=" * 50)
        print("–®–ê–ì 1: –ó–ê–ì–†–£–ó–ö–ê –ü–û–õ–ù–´–• –î–ê–ù–ù–´–•")
        print("=" * 50)
        signals = processor.get_all_signals_complete(limit=limit_signals)

        if signals.empty:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return

        # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("\n" + "=" * 50)
        print("–®–ê–ì 2: –†–ê–°–ß–ï–¢ –†–ê–°–®–ò–†–ï–ù–ù–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("=" * 50)
        signals_with_outcomes = processor.calculate_signal_outcomes_enhanced(signals, batch_size=100)

        if signals_with_outcomes.empty:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return

        # 3. –î–æ–±–∞–≤–ª—è–µ–º —Ä—ã–Ω–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω —Å–∫—Ä–∏–ø—Ç)
        print("\n" + "=" * 50)
        print("–®–ê–ì 3: –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –†–´–ù–û–ß–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê")
        print("=" * 50)
        try:
            from market_context_analyzer import MarketContextAnalyzer
            market_analyzer = MarketContextAnalyzer()
            enriched_signals = market_analyzer.enrich_signals_with_market_context(signals_with_outcomes)

            # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ä–µ–∂–∏–º–∞–º
            regime_performance = market_analyzer.analyze_performance_by_regime(enriched_signals)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Ä–µ–∂–∏–º–æ–≤
            with open('reports/market_regime_analysis_full.json', 'w') as f:
                json.dump(regime_performance, f, indent=2, default=str)

        except ImportError:
            print("‚ö†Ô∏è  –ú–æ–¥—É–ª—å market_context_analyzer –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
            enriched_signals = signals_with_outcomes

        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        print("\n" + "=" * 50)
        print("–®–ê–ì 4: –°–û–•–†–ê–ù–ï–ù–ò–ï –î–ê–ù–ù–´–•")
        print("=" * 50)
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        enriched_signals.to_pickle('full_signals_with_all_data.pkl')
        enriched_signals.to_csv('full_signals_with_all_data.csv', index=False)
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(enriched_signals)} —Å–∏–≥–Ω–∞–ª–æ–≤ —Å {len(enriched_signals.columns)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")

        # 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML
        print("\n" + "=" * 50)
        print("–®–ê–ì 5: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø ML")
        print("=" * 50)
        X, y, feature_names = processor.prepare_full_feature_set(enriched_signals)

        if X.empty or y.empty:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
            return

        # 6. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print("\n" + "=" * 50)
        print("–®–ê–ì 6: –û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ï–ô")
        print("=" * 50)
        ml_results = processor.train_enhanced_models(X, y)

        # 7. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≥—Ä—É–ø–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("\n" + "=" * 50)
        print("–®–ê–ì 7: –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í")
        print("=" * 50)
        group_importance = processor.analyze_feature_groups_importance(ml_results)

        # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        print("\n" + "=" * 50)
        print("–®–ê–ì 8: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        print("=" * 50)
        output_dir = 'models_full_data'
        os.makedirs(output_dir, exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        for model_name, model in processor.models.items():
            joblib.dump(model, os.path.join(output_dir, f'{model_name}_model.pkl'))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        joblib.dump(processor.scaler, os.path.join(output_dir, 'scaler.pkl'))

        with open(os.path.join(output_dir, 'feature_names.json'), 'w') as f:
            json.dump(feature_names, f)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        json_results = {
            model_name: {
                'accuracy': float(m['accuracy']),
                'auc': float(m['auc']),
                'precision_0': float(m['precision_0']),
                'recall_0': float(m['recall_0']),
                'precision_1': float(m['precision_1']),
                'recall_1': float(m['recall_1']),
                'f1_score': float(m['f1_score'])
            } for model_name, m in ml_results.items()
        }

        with open(os.path.join(output_dir, 'training_results.json'), 'w') as f:
            json.dump(json_results, f, indent=2)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if 'random_forest' in ml_results and 'feature_importance' in ml_results['random_forest']:
            ml_results['random_forest']['feature_importance'].to_csv(
                os.path.join(output_dir, 'feature_importance_full.csv'),
                index=False
            )

        if not group_importance.empty:
            group_importance.to_csv(
                os.path.join(output_dir, 'feature_group_importance.csv')
            )

        print(f"‚úÖ –ú–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{output_dir}/'")

        # 9. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        print("\n" + "=" * 50)
        print("–®–ê–ì 9: –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–ï–¢–ê")
        print("=" * 50)
        report_path = processor.generate_comprehensive_report(
            enriched_signals, ml_results, group_importance
        )

        # 10. –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "=" * 100)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print("=" * 100)
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   - –î–∞–Ω–Ω—ã–µ: full_signals_with_all_data.pkl/csv")
        print(f"   - –ú–æ–¥–µ–ª–∏: {output_dir}/")
        print(f"   - –û—Ç—á–µ—Ç: {report_path}")

        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        best_model_name, best_model_metrics = max(
            ml_results.items(),
            key=lambda item: item[1]['auc']
        )
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name.upper()}")
        print(f"   - AUC: {best_model_metrics['auc']:.4f}")
        print(f"   - F1-Score: {best_model_metrics['f1_score']:.3f}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º:")
        print(f"   - –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(enriched_signals)}")
        print(f"   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")
        print(f"   - –£—Å–ø–µ—à–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {(y == 1).sum()} ({(y == 1).sum() / len(y) * 100:.1f}%)")

        # –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'random_forest' in ml_results and 'top_features' in ml_results['random_forest']:
            print("\nüéØ –¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for idx, row in ml_results['random_forest']['top_features'].head(5).iterrows():
                print(f"   {idx + 1}. {row['feature']}: {row['importance']:.4f}")

    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()