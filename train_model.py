import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.impute import SimpleImputer

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
# –£–∫–∞–∂–∏—Ç–µ –∏–º—è –≤–∞—à–µ–≥–æ –ü–û–õ–ù–û–ì–û —Ñ–∞–π–ª–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
PARQUET_FILENAME = 'full_ai_dataset_2025-06-12_1543.parquet' # <--- –ó–ê–ú–ï–ù–ò–¢–ï –ù–ê –ò–ú–Ø –í–ê–®–ï–ì–û –ü–û–°–õ–ï–î–ù–ï–ì–û –§–ê–ô–õ–ê

# --- 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ---
try:
    df = pd.read_parquet(PARQUET_FILENAME)
    print(f"‚úÖ –§–∞–π–ª '{PARQUET_FILENAME}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(df)}")
except FileNotFoundError:
    print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª '{PARQUET_FILENAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ.")
    exit()

# –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
df.dropna(subset=['max_profit_pct', 'max_drawdown_pct'], inplace=True)
print(f"–û—Å—Ç–∞–ª–æ—Å—å —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(df)}")

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—à–µ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π 'y': –£—Å–ø–µ—à–Ω—ã–π —Å–∏–≥–Ω–∞–ª = (–ø—Ä–æ—Ñ–∏—Ç >= 5%) –ò (–ø—Ä–æ—Å–∞–¥–∫–∞ > -3%)
df['is_successful'] = ((df['did_hit_profit_target_5pct'] == 1) & (df['did_hit_stop_loss_3pct'] == 0)).astype(int)

# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ì–û–¢–û–í–ò–ú –ü–†–ò–ó–ù–ê–ö–ò –î–õ–Ø –ú–û–î–ï–õ–ò ---

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª—å (—Ç–æ, —á—Ç–æ –º—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º)
y = df['is_successful']

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (X), –∏—Å–ø–æ–ª—å–∑—É—è –í–°–ï –¥–∞–Ω–Ω—ã–µ, –∫—Ä–æ–º–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∏ —Ü–µ–ª–µ–≤—ã—Ö
# –ò—Å–∫–ª—é—á–∞–µ–º ID, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏, —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏ —Å–ª–æ–∂–Ω—ã–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
columns_to_drop = [
    'signal_id', 'symbol', 'timestamp', 'enriched_id', 'created_at', 'updated_at',
    'timeseries_to_max_profit', 'timeseries_to_stop_loss', # —Å–ª–æ–∂–Ω—ã–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    'is_successful', 'did_hit_profit_target_5pct', 'did_hit_stop_loss_3pct', # —Ü–µ–ª–µ–≤—ã–µ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –æ—Ç –Ω–∏—Ö
    'max_profit_pct', 'max_drawdown_pct', 'time_to_max_profit_minutes' # —Å–∞–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ–∂–µ –∏—Å–∫–ª—é—á–∞–µ–º –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
]
X_raw = df.drop(columns=columns_to_drop, errors='ignore')

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (—Ç–µ–∫—Å—Ç) –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç (One-Hot Encoding)
X = pd.get_dummies(X_raw, dummy_na=True)

# –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∏–º–µ–Ω–∞ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
feature_names = X.columns

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
imputer = SimpleImputer(strategy='median')
X = imputer.fit_transform(X)

print(f"\n–ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö.")
print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤:")
print(y.value_counts())


# --- 2. –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("\nüå≥ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö...")
model = RandomForestClassifier(n_estimators=150, random_state=42, class_weight='balanced', n_jobs=-1)
model.fit(X_train, y_train)
print("–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")


# --- 3. –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò ---
print("\n--- –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---")
y_pred = model.predict(X_test)

print("\n–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['–ù–µ—É—Å–ø–µ—à–Ω—ã–π', '–£—Å–ø–µ—à–Ω—ã–π'], yticklabels=['–ù–µ—É—Å–ø–µ—à–Ω—ã–π', '–£—Å–ø–µ—à–Ω—ã–π'])
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
plt.savefig('full_model_confusion_matrix.png')
print("üñºÔ∏è  –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'full_model_confusion_matrix.png'")
plt.clf()

print("\n–û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
print(classification_report(y_test, y_pred, target_names=['–ù–µ—É—Å–ø–µ—à–Ω—ã–π', '–£—Å–ø–µ—à–Ω—ã–π']))


# --- 4. –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í ---
print("\n--- –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è –º–æ–¥–µ–ª—å—é ---")
feature_importances = pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False)

print("–¢–û–ü-15 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
print(feature_importances.head(15))

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
plt.figure(figsize=(12, 10))
sns.barplot(x=feature_importances.head(20), y=feature_importances.head(20).index)
plt.title('–¢–û–ü-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–µ—Ä—Å–∏–∏ Random Forest')
plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫–∏')
plt.tight_layout()
plt.savefig('full_model_feature_importance.png')
print("\nüñºÔ∏è  –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'full_model_feature_importance.png'")
print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∏ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω—ã.")