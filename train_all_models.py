import pandas as pd
import numpy as np
import joblib
import sys
import os

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥–µ–ª–µ–π –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from dotenv import load_dotenv

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
PARQUET_FILENAME = 'full_ai_dataset_2025-06-12_1543.parquet' # <--- –£–ö–ê–ñ–ò–¢–ï –ò–ú–Ø –í–ê–®–ï–ì–û –ü–û–°–õ–ï–î–ù–ï–ì–û –§–ê–ô–õ–ê
dotenv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env')
load_dotenv(dotenv_path)

# --- 1. –ó–ê–ì–†–£–ó–ö–ê –ò –û–ë–©–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ---
try:
    df = pd.read_parquet(PARQUET_FILENAME)
    print(f"‚úÖ –§–∞–π–ª '{PARQUET_FILENAME}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(df)}")
except FileNotFoundError:
    print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª '{PARQUET_FILENAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    sys.exit(1)

df.dropna(subset=['max_profit_pct', 'max_drawdown_pct'], inplace=True)
print(f"–û—Å—Ç–∞–ª–æ—Å—å —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(df)}")

df['is_successful'] = ((df['did_hit_profit_target_5pct'] == 1) & (df['did_hit_stop_loss_3pct'] == 0)).astype(int)

y = df['is_successful']
columns_to_drop = [
    'signal_id', 'token_id', 'symbol', 'timestamp', 'enriched_id', 'created_at', 'updated_at',
    'timeseries_to_max_profit', 'timeseries_to_stop_loss',
    'is_successful', 'did_hit_profit_target_5pct', 'did_hit_stop_loss_3pct',
    'max_profit_pct', 'max_drawdown_pct', 'time_to_max_profit_minutes'
]
X = df.drop(columns=columns_to_drop, errors='ignore')

print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤:")
print(y.value_counts())

# --- 2. –°–û–ó–î–ê–ù–ò–ï –ö–û–ù–í–ï–ô–ï–†–û–í –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò ---

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫
numeric_features = X.select_dtypes(include=np.number).columns.to_list()
categorical_features = X.select_dtypes(include=['object']).columns.to_list()

# –ö–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –º–æ–¥–µ–ª–µ–π, —Ç—Ä–µ–±—É—é—â–∏—Ö One-Hot Encoding (RF, XGBoost)
# –û–Ω —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç –ø—Ä–æ–ø—É—Å–∫–∏, –∞ –ø–æ—Ç–æ–º –∑–∞–∫–æ–¥–∏—Ä—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
preprocessor_dummies = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='median'), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ],
    remainder='passthrough'
)

# –ö–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è LightGBM
# –û–Ω –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–ª–Ω–∏—Ç –ø—Ä–æ–ø—É—Å–∫–∏, —Ç–∞–∫ –∫–∞–∫ LGBM —Å–∞–º –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
# –í–ê–ñ–ù–û: –ú—ã –±—É–¥–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å —Ç–∏–ø—ã –ü–ï–†–ï–î –ø–µ—Ä–µ–¥–∞—á–µ–π –≤ –∫–æ–Ω–≤–µ–π–µ—Ä
preprocessor_lgbm = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='median'), numeric_features)
    ],
    remainder='passthrough'
)


# --- 3. –û–ë–£–ß–ï–ù–ò–ï –ò –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ---

X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# --- –ú–æ–¥–µ–ª—å 1: Random Forest ---
print("\nüå≥ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest...")
rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor_dummies),
                              ('classifier', RandomForestClassifier(n_estimators=150, random_state=42, class_weight='balanced', n_jobs=-1))])
rf_pipeline.fit(X_train, y_train)
joblib.dump(rf_pipeline, 'rf_pipeline.joblib')
print("‚úÖ –ö–æ–Ω–≤–µ–π–µ—Ä Random Forest –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")

# --- –ú–æ–¥–µ–ª—å 2: XGBoost ---
print("\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ XGBoost...")
class_counts = y.value_counts()
scale_pos_weight = class_counts[0] / class_counts[1] if class_counts[1] > 0 else 1
xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor_dummies),
                               ('classifier', xgb.XGBClassifier(objective='binary:logistic', n_estimators=150, learning_rate=0.1, max_depth=5, scale_pos_weight=scale_pos_weight, eval_metric='logloss', random_state=42, n_jobs=-1))])
xgb_pipeline.fit(X_train, y_train)
joblib.dump(xgb_pipeline, 'xgb_pipeline.joblib')
print("‚úÖ –ö–æ–Ω–≤–µ–π–µ—Ä XGBoost –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")

# --- –ú–æ–¥–µ–ª—å 3: LightGBM ---
print("\nüí° –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LightGBM...")
# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π –≤ –∫–æ–Ω–≤–µ–π–µ—Ä
X_train_lgbm = X_train.copy()
for col in categorical_features:
    X_train_lgbm[col] = X_train_lgbm[col].astype('category')

lgbm_pipeline = Pipeline(steps=[('preprocessor', preprocessor_lgbm),
                                ('classifier', lgb.LGBMClassifier(objective='binary', n_estimators=150, learning_rate=0.1, num_leaves=31, is_unbalance=True, random_state=42, n_jobs=-1))])
lgbm_pipeline.fit(X_train_lgbm, y_train) # –û–±—É—á–∞–µ–º –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç–∏–ø–æ–º
joblib.dump(lgbm_pipeline, 'lgbm_pipeline.joblib')
joblib.dump(categorical_features, 'lgbm_categorical_features.joblib') # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
print("‚úÖ –ö–æ–Ω–≤–µ–π–µ—Ä LightGBM –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")


print("\nüèÅ –í—Å–µ –º–æ–¥–µ–ª–∏ –≤ –≤–∏–¥–µ –∫–æ–Ω–≤–µ–π–µ—Ä–æ–≤ —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")

